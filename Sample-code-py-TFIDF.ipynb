{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1d67d0",
   "metadata": {},
   "source": [
    "# Pythonサンプルプログラム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9e2abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikit-learn nltk\n",
    "\n",
    "# 必要なNLTKリソースをダウンロード\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd606288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 情報検索\n",
    "\n",
    "def run_search_documents():\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    def search_documents(documents, search_words):\n",
    "\n",
    "        # TF-IDFベクトライザー\n",
    "        stop_words = stopwords.words('english')\n",
    "        vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "        # TF-IDFへ変換\n",
    "        tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "        search_words_vector = vectorizer.transform([search_words])\n",
    "\n",
    "        # コサイン類似度でソート\n",
    "        cosine_similarities = cosine_similarity(search_words_vector, tfidf_matrix).flatten()\n",
    "        sorted_indices = np.argsort(-cosine_similarities)\n",
    "\n",
    "        # 結果を(スコア, ドキュメント)の形式で返す\n",
    "        results = [(cosine_similarities[i], documents[i]) for i in sorted_indices]\n",
    "\n",
    "        return results\n",
    "\n",
    "    print(\"# 情報検索\")\n",
    "\n",
    "    # サンプルドキュメント\n",
    "    documents = [\n",
    "        \"The stock market is experiencing unprecedented growth.\",\n",
    "        \"Local football team wins the championship.\",\n",
    "        \"New advancements in AI technology are transforming the industry.\",\n",
    "        \"The government announces new economic policies.\",\n",
    "        \"Celebrity couple announces their engagement.\",\n",
    "        \"New study reveals health benefits of a balanced diet.\"\n",
    "    ]\n",
    "\n",
    "    # 検索ワード\n",
    "    query = \"AI technology\"\n",
    "\n",
    "    # 検索の実行\n",
    "    results = search_documents(documents, query)\n",
    "\n",
    "    # 結果の表示\n",
    "    for score, doc in results:\n",
    "        print(f\"Score: {score:.4f}, Document: {doc}\")\n",
    "\n",
    "# # Test\n",
    "# run_search_documents()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e3ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 情報フィルタリング\n",
    "\n",
    "def run_information_filtering():\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    def filter_information(documents, search_words):\n",
    "\n",
    "        # TF-IDFベクトライザー\n",
    "        stop_words = stopwords.words('english')\n",
    "        vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "        # TF-IDFへ変換\n",
    "        tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "        search_words_vector = vectorizer.transform([search_words])\n",
    "\n",
    "        # コサイン類似度でソート\n",
    "        cosine_similarities = cosine_similarity(search_words_vector, tfidf_matrix).flatten()\n",
    "        sorted_indices = np.argsort(-cosine_similarities)\n",
    "\n",
    "        # 結果を(スコア, ドキュメント)の形式で返す\n",
    "        results = [(cosine_similarities[i], documents[i]) for i in sorted_indices]\n",
    "\n",
    "        return results\n",
    "\n",
    "    print(\"# 情報フィルタリング（TF-IDF）\")\n",
    "\n",
    "    # サンプルニュース記事\n",
    "    documents = [\n",
    "        \"The stock market is experiencing unprecedented growth.\",\n",
    "        \"Local football team wins the championship.\",\n",
    "        \"New advancements in AI technology are transforming the industry.\",\n",
    "        \"The government announces new economic policies.\",\n",
    "        \"Celebrity couple announces their engagement.\",\n",
    "        \"New study reveals health benefits of a balanced diet.\"\n",
    "    ]\n",
    "\n",
    "    # 検索ワード\n",
    "    search_word = \"AI technology and economic policies\"\n",
    "\n",
    "    # 情報フィルタリングの実行\n",
    "    results = filter_information(documents, search_word)\n",
    "\n",
    "    # 結果の表示\n",
    "    for score, doc in results:\n",
    "        print(f\"Score: {score:.4f}, Document: {doc}\")\n",
    "\n",
    "# # Test\n",
    "# run_information_filtering()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9de402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 情報フィルタリング\n",
    "\n",
    "def run_information_filtering_word2vec():\n",
    "    from gensim.models import Word2Vec\n",
    "    from gensim.utils import simple_preprocess\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    # ドキュメントをWord2Vecベクトルに変換する関数\n",
    "    def documents_to_vectors(documents, model):\n",
    "        vectors = []\n",
    "        for document in documents:\n",
    "            words = word_tokenize(document.lower())\n",
    "            words = [word for word in words if word.isalnum()]\n",
    "            word_vectors = [model.wv[word] for word in words if word in model.wv.key_to_index]\n",
    "            if word_vectors:\n",
    "                vectors.append(np.mean(word_vectors, axis=0))\n",
    "            else:\n",
    "                vectors.append(np.zeros(model.vector_size))\n",
    "        return np.array(vectors)\n",
    "\n",
    "    def filter_information(documents, search_words, model):\n",
    "        # ドキュメントをベクトルに変換\n",
    "        doc_vectors = documents_to_vectors(documents, model)\n",
    "        \n",
    "        # 検索ワードをベクトルに変換\n",
    "        search_words_vector = documents_to_vectors([search_words], model)\n",
    "\n",
    "        # コサイン類似度でソート\n",
    "        cosine_similarities = cosine_similarity(search_words_vector, doc_vectors).flatten()\n",
    "        sorted_indices = np.argsort(-cosine_similarities)\n",
    "\n",
    "        # 結果を(スコア, ドキュメント)の形式で返す\n",
    "        results = [(cosine_similarities[i], documents[i]) for i in sorted_indices]\n",
    "\n",
    "        return results\n",
    "\n",
    "    print(\"# 情報フィルタリング（Word2Vec）\")\n",
    "\n",
    "    # サンプルニュース記事\n",
    "    documents = [\n",
    "        \"The stock market is experiencing unprecedented growth.\",\n",
    "        \"Local football team wins the championship.\",\n",
    "        \"New advancements in AI technology are transforming the industry.\",\n",
    "        \"The government announces new economic policies.\",\n",
    "        \"Celebrity couple announces their engagement.\",\n",
    "        \"New study reveals health benefits of a balanced diet.\"\n",
    "    ]\n",
    "\n",
    "    # 検索ワード\n",
    "    search_word = \"AI technology and economic policies\"\n",
    "\n",
    "    # Word2Vecモデルのトレーニング（またはロード）\n",
    "    tokenized_documents = [word_tokenize(doc.lower()) for doc in documents]\n",
    "    model = Word2Vec(tokenized_documents, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "    # 情報フィルタリングの実行\n",
    "    results = filter_information(documents, search_word, model)\n",
    "\n",
    "    # 結果の表示\n",
    "    for score, doc in results:\n",
    "        print(f\"Score: {score:.4f}, Document: {doc}\")\n",
    "\n",
    "# # Test\n",
    "# run_information_filtering_word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ebad364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 情報フィルタリング\n",
    "\n",
    "def run_information_filtering_bert():\n",
    "    from transformers import BertTokenizer, BertModel\n",
    "    import torch\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    # BERTモデルとトークナイザーのロード\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # ドキュメントをBERTベクトルに変換する関数\n",
    "    def documents_to_vectors(documents):\n",
    "        vectors = []\n",
    "        for document in documents:\n",
    "            inputs = tokenizer(document, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            # CLSトークンのベクトルを使用\n",
    "            cls_vector = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "            vectors.append(cls_vector.flatten())\n",
    "        return np.array(vectors)\n",
    "\n",
    "    def filter_information(documents, search_words):\n",
    "        # ドキュメントをベクトルに変換\n",
    "        doc_vectors = documents_to_vectors(documents)\n",
    "        \n",
    "        # 検索ワードをベクトルに変換\n",
    "        search_words_vector = documents_to_vectors([search_words])\n",
    "\n",
    "        # コサイン類似度でソート\n",
    "        cosine_similarities = cosine_similarity(search_words_vector, doc_vectors).flatten()\n",
    "        sorted_indices = np.argsort(-cosine_similarities)\n",
    "\n",
    "        # 結果を(スコア, ドキュメント)の形式で返す\n",
    "        results = [(cosine_similarities[i], documents[i]) for i in sorted_indices]\n",
    "\n",
    "        return results\n",
    "\n",
    "    print(\"# 情報フィルタリング（BERT）\")\n",
    "\n",
    "    # サンプルニュース記事\n",
    "    documents = [\n",
    "        \"The stock market is experiencing unprecedented growth.\",\n",
    "        \"Local football team wins the championship.\",\n",
    "        \"New advancements in AI technology are transforming the industry.\",\n",
    "        \"The government announces new economic policies.\",\n",
    "        \"Celebrity couple announces their engagement.\",\n",
    "        \"New study reveals health benefits of a balanced diet.\"\n",
    "    ]\n",
    "\n",
    "    # 検索ワード\n",
    "    search_word = \"AI technology and economic policies\"\n",
    "\n",
    "    # 情報フィルタリングの実行\n",
    "    results = filter_information(documents, search_word)\n",
    "\n",
    "    # 結果の表示\n",
    "    for score, doc in results:\n",
    "        print(f\"Score: {score:.4f}, Document: {doc}\")\n",
    "\n",
    "# # Test\n",
    "# run_information_filtering_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "172af7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # 情報検索\n",
    "    run_search_documents()\n",
    "\n",
    "    # 情報フィルタリング\n",
    "    run_information_filtering()\n",
    "    \n",
    "    # 情報フィルタリング(Word2Vec)\n",
    "    run_information_filtering_word2vec()\n",
    "    \n",
    "    # 情報フィルタリング(BERT)\n",
    "    run_information_filtering_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "310212ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 情報検索\n",
      "Score: 0.6042, Document: New advancements in AI technology are transforming the industry.\n",
      "Score: 0.0000, Document: The stock market is experiencing unprecedented growth.\n",
      "Score: 0.0000, Document: Local football team wins the championship.\n",
      "Score: 0.0000, Document: The government announces new economic policies.\n",
      "Score: 0.0000, Document: Celebrity couple announces their engagement.\n",
      "Score: 0.0000, Document: New study reveals health benefits of a balanced diet.\n",
      "# 情報フィルタリング（TF-IDF）\n",
      "Score: 0.4908, Document: The government announces new economic policies.\n",
      "Score: 0.4272, Document: New advancements in AI technology are transforming the industry.\n",
      "Score: 0.0000, Document: The stock market is experiencing unprecedented growth.\n",
      "Score: 0.0000, Document: Local football team wins the championship.\n",
      "Score: 0.0000, Document: Celebrity couple announces their engagement.\n",
      "Score: 0.0000, Document: New study reveals health benefits of a balanced diet.\n",
      "# 情報フィルタリング（Word2Vec）\n",
      "Score: 0.5785, Document: The government announces new economic policies.\n",
      "Score: 0.4648, Document: New advancements in AI technology are transforming the industry.\n",
      "Score: 0.1451, Document: Local football team wins the championship.\n",
      "Score: 0.0171, Document: New study reveals health benefits of a balanced diet.\n",
      "Score: -0.0151, Document: The stock market is experiencing unprecedented growth.\n",
      "Score: -0.1542, Document: Celebrity couple announces their engagement.\n",
      "# 情報フィルタリング（BERT）\n",
      "Score: 0.8143, Document: New study reveals health benefits of a balanced diet.\n",
      "Score: 0.7850, Document: New advancements in AI technology are transforming the industry.\n",
      "Score: 0.7849, Document: The government announces new economic policies.\n",
      "Score: 0.7837, Document: The stock market is experiencing unprecedented growth.\n",
      "Score: 0.7209, Document: Local football team wins the championship.\n",
      "Score: 0.6899, Document: Celebrity couple announces their engagement.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
